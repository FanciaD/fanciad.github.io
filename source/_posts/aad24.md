---
title: 算法分析与设计2024 笔记
date: '2024-12-29 23:19:44'
updated: '2024-12-29 23:19:44'
tags: Fancia
permalink: ForThoseWhoRemain/
description: 算法分析与设计2024
mathjax: true
---

## Algorithms - 2024 Autumn

> 显然这是一篇中文 note。

### I Undirected APSP Approximation

著名的 APSP Conjecture 说，精确解 APSP 不能在 $O(n^{3-\epsilon})$ 中解决。那近似能不能做到更好呢？

一些简单的背景知识：

我们默认 $m=O(n^2)$。

APSP 的直接做法：$O(n^3)$

SSSP(单源) 的直接做法：$O((n+m)\log n)$（各种优先队列）/ $O(m+n\log n)$（斐波那契堆），但这里我们一般只看 $n$ 上面的指数，所以都可以写成 $\tilde O(n+m)$，这个记号表示忽略 $\log$ factor。

对于近似算法，记 $\delta$ 为真实距离，我们要求对于每一对点，估计的距离满足 $\delta\mathbf{\leq} d\leq a*\delta+b$。那么这被称为 $(*a,+b)$ 的近似，或者简记作 $(a,b)$。

这里主要考虑无向情况，下面会省略这一点。

#### Additive, Unweighted

> Additive: (*1,+c)

那这东西有边权显然没啥道理，所以考虑无边权情况。

##### n^2.5 +2

这里几乎所有的做法都从这样一个角度入手：注意到做 $m$ 次 SSSP 是 $\tilde O(nm)$ 的，如果 $m$ 很小就显然比 $n^3$ 快。那能不能特殊处理度数很大的点？那正因为它们度数很大，有如下性质：

考虑随机一些点，以 $1/k$ 的概率随机选每个点（期望选 $n/k$ 个），那此时对于度数 $O(k\log n)$ 的点，它与一个选中点相邻的概率就是 $n^{-O(1)}$，那么高概率每个度数这么大的点都和一个选中点相邻。

更近一步还可以确定性构造 $O(k\log n)$ 的 Greedy Set Cover，但是我忘了。

考虑 $k=\sqrt n$，然后求出这 $O(n/k)$ 个点到其它点的最短路（$O(n^{2.5})$）。此时对于所有度数为 $O(k\log n)$ 的点，它们都和某个选中点相邻，那它们走选中点去近似最多距离多 $2$：只绕了一条边。

然后考虑处理剩下的点，它们走到度数为 $O(k\log n)$ 的点后就可以用上面的近似，而它们之间只有 $\tilde O(nk)$ 条边。那可以直接考虑 APSP：保留小度数之间和小到大的边（$\tilde O(nk)$ 条），保留大度数的点到一个给定点的边，然后给定点往外的最短路可以最短路树或者直接 $n$ 条边。这样每一步都是 $\tilde O(n^{2.5})$。

还有一个更简单的想法：

1. 如果 $u$ 到 $v$ 路径上经过度数 $O(k\log n)$ 的点，那么最短路与某个选中点相邻，可以直接用选中点到 $u,v$ 的最短路拼起来更新。这部分复杂度 $\tilde O(n^3/k)$。
2. 否则，只保留度数小的点做 APSP，复杂度 $\tilde O(n^2k)$。

##### Naive layering

回到上一个做法，最后一步使用选中点到其它点的距离更新。这里能不能继续近似？

选一个 $k$，按照度数 $n^{1/k},n^{2/k},\cdots$ 分层，每一层选 $ \tilde O(n/d)$ 个点。可以认为第 $0$ 层是 $d=1$。

从后往前近似每一层选中点的 SSSP。对于每一层，以下一层为关键点（用之前的近似距离）做上述算法。对于 $n^{i/k}$ 的一层，这一层点数为 $n^{1-i/k}$，下一层关键点度数为 $n^{(i+1)/k}$，因此边数很少……吗？一个问题是我们要从关键点跳到目标点，这是 $O(n^{1-(i+1)/k}*n)$ 的。

但这里可以倒过来想！考虑起点先跳到一个关键点（或者不跳），然后走原先的边。这样单起点的时候就只需要加这个点到关键点的距离，这样的边数就不到 $O(n)$。然后就可以继续分析，复杂度 $\tilde O(n^{2+1/k})$，近似为 $+2(k-1)$。

取 $O(\log n)$ 层就是 $\tilde O(n^2)$，近似 $+O(\log n)$。

但我们还有更好的分层常数，首先我们要回到 $+2$ 看一下。

##### n^7/3 +2

考虑刚才的分 $3$ 层做法，直接分析得到的是 $+4$，但能做到更好吗？考虑 $u$ 到 $v$ 的最短路，如果它只经过前两层（度数小于 $n^{2/3}$）的点，那上面只近似了一层，从而近似是 $+2$。那如果经过了最大一层的点呢？考虑再加下面一步：

3. 拿出第三层的选中点，枚举 $u,v$ 通过关键点更新距离，这样如果经过了第三层，仍然得到 $+2$ 近似。

这样就在 $+2$ 做到了 $7/3$。

##### +2 With Matrix Multiplication

众所周知 $(\min,+)$ 矩阵乘法也是非常难的，但特殊情况下可以是比较简单的。

比如，如果一个矩阵的每一行相邻差分都是有界(常数)的，那可以通过一些技巧做到 $O(n^{(2+\omega)/2})$ [Chi, Duan, Xie, Zhang 2022] (咕了)

能用到这里的第三步吗？那个显然是 $n\times k\times n$ 的乘法。一个问题是这里不一定差分有界，但我们可以：

考虑找一个 Tour，允许点经过多次，那长度可以不超过 $2n$。此时相邻两个点有边相连，那任何原点到它们的距离差都不超过 $1$。

然后就可以考虑直接套了，但小问题是求第三层到其它点的距离也是不矩阵乘法的复杂度，所以这里还需要优化。

考虑上面的 多层 Layering 和优化：注意到如果我们在每一层都跑一次 $n\times k\times n$，那显然能更新出 $+2$ 的答案：找到最短路所在的最高层，那一层上跑的就是对的。如果每一层只是 $d$ 乘常数，那跑最短路的部分是 $\tilde O(n^2)$ 的。矩阵乘法部分优化一下，就可以去掉到其它点的距离这部分了。

##### Improvement of Layering

Dor-Halperin-Zwick 96: $+2k,n^{2+1/(3k-1)}$。

在 $7/3$ 那里我们干了啥？就只是加入了顶层到其它所有层的边。这样如果第三层要从顶层转，那这样也是 $+2$。

为了方便，我们换一个编号顺序：第 $i$ 层包含 $n^{i/k}$ 个点，这样本来是顺序做。

那能不能多来几次？考虑把能加的都加进来：对于点数 $n^{i/k}$ 的一层，我们枚举层数对，如果乘起来不超过 $n^{2-(i-1)/k}$，那么加进来就不影响复杂度，然后全部加进来。

然后我们考虑第 $a$ 层的点到第 $b$ 层的点的最短路($a\leq b$)，然后假设边最多留到了第 $c$ 层。那如果 $a+b+c\leq 2k+1$，我们在做第 $i$ 层的时候就有 $a\to c\to b$ 的跳边，从而只有 $+2$ 的代价。

那直接想，对于 $a\leq k/2$，第 $a$ 层出发的路径近似都不超过 $+2$，后面再做就是 $k$ 层，$+k(+c)$。

能不能更好？继续分析，考虑找到 $a$ 到 $b$ 路径上最后一个层数 $>a$ 的关键点，这是一种方式是我们先近似那个距离，然后直接走到 $b$（从 $a$ 这一层开始），这就一步变成了 $a=c$ 的情况，然后 $a\leq 2k/3$ 都是好的。从而只有在这之后近似代价才从常数开始增加。这样就是 $k$ 层，$+2/3k(+c)$。然后就是上面的东西。 

#### Multiplicative

这个就不那么关心边权了。

让我们拿出一堆做图上距离近似的常见手法，比如考虑一个点的邻域而不仅仅是直接相邻点。

首先考虑怎么求 $k$-邻域。一个问题是边数 $m$ 非常大。但显然每个点出边只有前 $k$ 条需要考虑，那容易搞到 $\tilde O(k^2)$。

#### n^2, *3

考虑怎么用一个 $k$-邻域。那经典操作还是撒点：随机 $O(n/k\log n)$ 个点，然后期望每个点的 $k$-邻域都高概率被 Cover 到，然后就考虑选中点求 SSSP($\tilde O(n^3/k)$)，然后每个点从邻域里面选中点更新距离。

这样额外距离是邻域半径的两倍（绕一圈）。注意到邻域内的可以直接求出来，那外面的近似就最多是 $*3$。

复杂度 $\tilde O(n^3/k+nk^2)$，那就是 $n^{7/3}$。

#### Cohen-Zwick 97

##### n^5/2, *2

能不能把之前的 $+2$ 做法拿过来？最直接的问题是，现在 $+2$ 变成了二倍额外边的边权，但额外边不在最短路内，所以直接加根本没有保证倍数。因此首先我们需要考虑：如何让额外边边权一定不超过最短路内的边权？

这是可以做到的：考虑把每个点的相邻边按照边权从小到大排序，然后：

1. 原本我们是对于度数小的点保留全部出边，度数大的点啥都不做；现在我们对于度数大的点，也保留它的前 $d$ 小出边。
2. 在覆盖时，原本我们考虑覆盖度数大的点的任意相邻点；现在只覆盖前 $d$ 小的出边。

显然两个修改都不改变复杂度。

考虑现在会发生什么。如果最短路和一个关键点相邻，且后面走不过去了，那连到关键点的边边权小于走不过去的边。这是因为覆盖邻域只考虑小的边，而小的边是不被删的。那如果直接用 $+2$ 的做法，我们就得到了 $n^{2.5}$ 的 $*3$ 近似……但这还打不过上一个算法。

显然这样的极限情况是只有一条边被删。可以发现如果有两条边被删，那这个算法都是很优秀的：如果有两条不同的边被删，那找到路径上第一个和最后一个，从小的那边跳过去就是 $*2$ 的近似。

因此只需要额外处理如下情况：最短路上正好有一条边被删掉。问题是我们也不能大力做这条边，不然还是一个 $(\min,+)$ 矩阵乘法。

但仔细想想这真的能做：我们需要先求出最后一步走额外边的最短路，然后继续向前扩展。“向前扩展”是 $\tilde O(nm)$ 的。

1. 求出第一步可以走额外边的最短路，那这就是每个原点加 $n$ 条边然后 Dij。
2. **翻过来**就是最后一步可以走额外边的最短路。
3. 然后求前半段是上面的东西，后半段走原先边的最短路。这就是原点可以先用上面的 dis 跳一次（加 $n$ 条边），然后做 Dij。

另一个极其简单的描述：先初始化 $d(i,j)$ 为额外边权，然后

1. 对于每个 $u$，加入 $n$ 条临时边 $u<->v, d(u,v)$，然后 SSSP 更新 $d$。
2. 对于每个 $u$，加入 $n$ 条临时边 $u<->v, d(u,v)$，然后 SSSP 更新 $d$。

对，就是做两遍。只需要记住更新 $d(u,v)$ 时同时更新 $d(v,u)$ 即可（我们考虑无向情况）。

原先我们需要做一遍原图上关键点起点的 DFS，再做一遍限制度数的图上的 APSP。这里需要做两遍限制度数的 APSP。那合起来即可。另一个简单描述是：

3. 做两次之前的算法。在算法中，每一步 SSSP 都加上这个点到其它点的 $d$ 这 $n$ 条边（初始化为所有的边权）。

##### n^7/3, *7/3

我们继续考虑套之前的做法；下一步是分 $n^{1/3},n^{2/3}$。回忆一下：（以下忽略任何 $\log$ 因子）

1. 第一层我们保留每个点前 $n^{2/3}$ 条出边，然后选 $n^{1/3}$ 个关键点覆盖每个大邻域（的前 $n^{2/3}$ 条出边）
2. 第二层我们保留前 $n^{1/3}$ 条，然后选 $n^{2/3}$ 个关键点。

原先的做法是，第一层的关键点在原图更新一遍，然后第二层的关键点在第一层的图(加上关键点)里面更新一遍，然后所有点在第二层的图里面更新一遍。然后我们讨论最短路上哪条边最先被删掉。那现在我们继续考虑：

1. 如果所有边都在第一层里面，那我们可以忽略第一层关键点的操作，因为第二层求出来都是准的。我们需要考虑有哪些边在第二层被删掉了。那这部分和之前两层的讨论是一样的：如果都在第二层，那直接搞出来就是准的；如果有两条边被删了，那从更短的那条那边去跳关键点，代价不超过两倍较短那条边长，所以是二倍；如果是只有一条被删了，那之前那样做两次就能解决。
2. 现在考虑有边被第一层删掉了。那考虑类似之前的操作，先考虑删了两条的情况，此时可以走较短的一侧过去，然后再跳过去做到两倍……吗？一个问题是我们在第二层上做这件事的时候，可能中间又删掉了一些边。但因为我们求出了第一层关键点到所有点的距离，所以可以跳两次：在后面每一步 SSSP 的时候，**额外加入第一层关键点到其它点的距离**（这是 $n^{4/3}$ 条边）。跳过去的步骤已经在处理两层一条的时候解决了。
3. 然后考虑第一层删了正好一条边的情况。如果第二层没删边，那做两次就搞定了。现在考虑关键情况：第二层又删了一些边。首先考虑又删了一条边情况，记当前图为 $u-...-a-b-...-c-d-...-v$，其中 $a-b$ 是第一层删的边 $e_1$，$c-d$ 是第二层删的边 $e_2$。我们记相邻点连到的关键点为 $v_{a/b/c/d}$。我们当然希望能通过 $c-d$ 两侧的关键点类似的更新：先跳到 $v_d$，再走剩余的部分（如果再跳，需要 $n^{5/3}$ 的边，不能接受）。那我们需要考虑能不能跳 $v_d$，或者说能不能 $v_d$ 求出到 $u$ 的最短路。问题是这里有一条删边，不能直接做。但只有一条的话，可以考虑之前的做法：第一轮求出 $a$ 跳到 $b$ 再往后走的距离，然后反过来推回去。但这里 $a$ **不是**关键点，所以不能过 $c-d$，这里只能走到 $v_c$ 而非 $v_d$。因此第二轮我们求出了 $v_c$ 到 $u$ 的最短路，而非 $v_d$。那怎么跳过去？考虑先跳到 $v_d$：记录所有第二层关键点间两两的最短路，这是 $n^{4/3}$ 的。但这样就要跳两次，更坏的是它们可能都对应一条边 $c-d$。所以代价是 $\min(2e_{ab},4e_{cd})$，那么得到 $7/3$。可以发现删更多可以一样做：左边有不影响 $v_c$ 推过去（因为它在第二层），右边有的话直接 $v_c$ 跳到最后一个 $v_f$ 即可。甚至这种情况下变回了 $*2$，因此最难的情况是删两条 $1-2$ 或者三条 $2-1-2$ 的情况。但这里很难编出类似之前的做法了。

总的来说，我们需要如下修改：

1. 加入第一层关键点到其它点的距离
2. 加入第二层关键点之间的距离

然后跑两轮。

##### n^2, *3

直接抄 $(*2,+1)$。

#### Distance Oracle

看成 DS 问题：不考虑预处理，考虑询问复杂度，近似比和**空间**占用。那显然空间目标比 $n^2$ 低。

例如，如果是上面的做法，那只需要记录邻域表和距离表，就是 $O(n^2/k+nk)$ 空间，$O(1)$ 时间，$3x$ 近似的数据结构。

##### Thorup, Zwick 01

考虑上一个做法，我们空间的瓶颈在于既要存邻域，又要存关键点到其它点的距离。能不能把第二步省下来？

首先，我们对于每个点存大小 $O(s\log n)$ 的邻域，然后随 $n/s$ 个关键点。此时邻域内部容易解决，邻域外部如果用邻域内的关键点去近似就是 $*3$。但这样需要 $n^2/s$ 空间记录距离。

考虑继续嵌套：对于每个点，只记录最近的 $O(s\log n)$ 个关键点。如果询问碰到这些点就直接回答，否则寄。那咋办呢？类似之前的思路，记录 $n/s^2$ 个关键点$^2$，那么外面的部分到它的距离都比更近的关键点$^2$大。

考虑如果记录了关键点$^2$到所有点的距离，此时近似比是多少。记路径是 $u\to v$。第一步，我们找到 $u$ 邻域里面的一个关键点 $p_1$。第二步，我们换到 $v$ 一侧，找到最近的关键点$^2$ $p_2$，此时 $d(p_2,v)\leq d(p_1,v)$，但后者是什么？只能得到 $\leq d(p_1,u)+d(u,v)\leq 2d(u,v)$。所以这是 $*5$ 近似。但空间变成了 $\tilde O(ns+n^2/s^2)$。

然后就类似了：记录关键点$^3,^4,\cdots$。每一层记录最近的 $O(s\log n)$ 个关键点，再记录最近的下一层关键点。询问就是来回嵌套：如果 $v$ 在 $u$ 的 $0$ 层邻域内就直接回答，否则找 $u$ 的关键点$^1$，看它在不在 $v$ 的 $1$ 层邻域内；如果不在，那找到 $v$ 最近的关键点$^2$，看在不在 $u$ 的 $2$ 层邻域内；那找到 $u$ 最近的关键点$^3$，看在不在 $v$ 的 $2$ 层邻域内……空间 $\tilde O(ns\log_s n)$，询问复杂度 $O(\log_s n)$，近似比 $2\log_s n+1$。取 $s=n^{1/k}$ 即可。

#### (2,1)-APSP

大概是把两种思路合并起来。

首先考虑之前的分 $O(\log n)$ 层。如果我们想做 $+2$ 的近似，那可以每一层里面，我们找到一大堆关键点，并求出它们到所有点的距离（这是 $\tilde O(n^2)$ 的）后，可以枚举每个选中点和起点终点更新。但直接更新复杂度不正确。

回忆一下问题，此时有一些（随的）关键点，然后我们需要考虑更新的最短路是路径在当前图中，且和至少一个关键点相邻。

现在考虑邻域技术：对于每个点，我们找到离它最近的关键点（多源 BFS），然后从那个点更新。这样额外代价是过去距离的两倍。

但我们注意到，我们要更新的最短路和至少一个关键点相邻。那么走最近的一侧过去近似，代价总是被控制住的。最坏情况是相邻在中点，此时是 $(*2,+2)$……吗？注意到如果只有一个点相邻，那因为小到大的边也存在，此时我们可以到下一层去考虑它，直到最短路上有连续**两个**大度数点。那中点就可以偏移一点了。最坏情况是奇数度数下占据两个中心，此时是 $(*2,+1)$。

Open Problem：做 $(*2,+0)$ 的近似，$\tilde O(n^2)$，现在用矩阵乘法做到了 2.03 啥的。

### II Heaps

一个数据结构，维护一堆有序的元素（我们先考虑比较模型，但用 $\Z$ 做例子），支持如下操作：

1. 插入元素
2. 查询最小值/删除最小值
3. **减小**元素的权值（可以认为每个元素有一个下标）
4. 合并两个数据结构

为什么是减小？这个东西非常常用，比如在写 Dijkstra 的时候，我们有 $n$ 次取出最小值，和 $m$ 次更新。在 $n,m$ 同阶时把更新当成插入做没有大问题，但 $m$ 更大时这样就慢了。



#### Binary Heap

考虑一个满二叉树：从上往下加节点，每一层从左往右依次加。

每个点上存一个元素，要求如下性质：

1. 父亲的权值不大于儿子的权值。

##### Insert

根据满二叉树的性质，我们插入的位置是固定的，但直接插入后不一定满足性质。

考虑插入后进行向上调整：如果当前点权小于父亲点权，就交换两元素，然后继续向上考虑。直观上这个过程只会不断向上，实际上：

1. 往上走的元素显然不会比新的儿子更大：原来的元素就不比它的儿子大，换上去只会来一个更小的元素。
2. 往下换的元素也不会比新的儿子更大：叶子的情况是显然的。对于更上面的情况，假设原来树上是 $u-v-...$，然后一个 $a$ 从下面过来依次换了 $v,u$。那这最后也是把 $u$ 换到了 $v$，而 $u$ 比 $v$ 小。这次换的链上部分是显然的。

那么复杂度 $O(depth)=O(\log n)$

##### Decrease Key

考虑减完直接往上 pop，和上面的分析一样做即可。复杂度 $O(depth)=O(\log n)$

##### Find/Delete Min

由上述性质，最小值显然在根。

考虑怎么删掉根。根据满二叉树我们本来应该删掉最后一个叶子。考虑 swap 过去然后把叶子删了，这相当于我们改了根节点的权值，此时它可能比儿子要大。

考虑向下调整：找一个最小的儿子向下交换，直到满足条件。类似的，换上来的点不会比另一侧大（选的更小的），也不会比父亲大（现在的父亲原先也是它的父亲）

复杂度 $O(depth)=O(\log n)$

##### Union

很遗憾这不能做，当然启发式合并一下好像可以说是均摊 $O(\log^2 n)$ 的。



#### Binomial Heap

考虑解决不能 union 的问题。

我们把满二叉树换成下面这样的二项树：

1. $B_0$ 是一个单点作为根。
2. $B_i$ 是一个 $B_{i-1}$，然后根再接一个 $B_{i-1}$ 作为子树。

图🕊了。

显然 $|B_i|=2^i$。另一件事是根正好有 $i$ 个儿子，因此这里 depth 和 degree 是等价的。但这也说明我们不能简单的维护树的结构。一种方式是把所有儿子按照 degree 从大到小维护，这样访问元素还是 $O(\log n)$，细节不考虑。

我们的元素数量不一定是 $2^n$，但我们可以二进制拆分到一个好看的形式，那就是一串 $B_i$。

##### Union

先考虑合并两个 $B_i$。注意到 $B_{i+1}$ 就是把两个 $B_i$ 的根连起来，那这个事情非常简单：比较两个根的大小，然后大的连向小的。

然后考虑合并两串 $B_i$，那就做一个二进制加法。

复杂度 $O(\log n)$

##### Insert

那就做一个 $+1$ 的 Union，复杂度 $O(\log n)$

##### Find/Delete Min

直接的方式：扫一遍根然后询问答案。

然后考虑怎么删。注意到 $B_i$ 拿掉根以后正好是 $B_{i-1}+B_{i-2}+\cdots+B_0$，那可以删完和外面做一个 Union。

复杂度 $O(\log n)$。这里可以用一点小技巧把 Find 变成 $O(1)$：别的操作后存下最小值。这在接下来还有用。

##### Decrease Key

那就学习 Binary Heap 的操作往上 pop，复杂度 $O(depth)=O(\log n)$

总的来说这个就比之前多了一个支持合并。



#### Fibonacci Heap

我们对二项堆进行如下修改：

1. 我们允许很多棵树的存在，只在某些时候合并。为了询问最小值，我们维护一个最小值的指针。
2. 为了更快地 Deckey，我们允许从树上删一个子树下来，使其变为新的一棵树。
3. 由于上述原因，合并时采用根节点的 degree 而非 depth。

具体来说，我们进行如下操作：



##### Insert

直接插入一个新的树 $B_0$，并更新指针。$O(1)$

##### Union

直接合并两侧所有树。考虑用双向链表维护，这样合并/接下来的插入删除都单词 $O(1)$。

##### Find/Remove Min

查询直接使用指针。对于删除最小值的步骤，我们类似二项堆的操作把它的子树全部放进来。这一步复杂度 $O(degree)$。

然后进行唯一的合并操作：将一大堆不同 degree 的树按照 degree 进行二项树合并。复杂度是 $O(\#trees+degree)$（维护略微复杂，但不困难）

##### Decrease Key

我们想要 $O(1)$ 复杂度，因此只能做如下操作：直接把修改点的子树从原树中拿出来，作为新的子树。



但这样怎么可能对？随便删可能导致任意的树结构，这会导致复杂度爆炸。因此我们采用如下方式：

> 每个非根节点最多丢失一个儿子，如果一次操作删掉了第二个，那我们马上把它也拿出来。

具体来说，每次上述操作时，我们拿出当前的子树，然后标记它的父亲。如果一个非根的父亲已经被标记过了，那我们即刻把它也删了，然后继续向上考虑。



显然这个最坏复杂度是不保证的：按照 degree 合并，每次插入三个->删一个合并另外两个造出大小2的链->插入删除把链合并上去->Deckey再删把额外部分删掉，我们就可以造出任意长度的链。因此我们需要：



##### Amortized Analysis

我们进行一些常见的势能分析。定义势能为 $t+2m$，其中 $t$ 为树的数量，$m$ 为标记的数量。

对于势能分析，我们可以将每一步操作的复杂度加上势能变化量作为均摊复杂度，同时需要势能非负。

然后考虑

1. 对于 Decrease Key 操作，我们每一步增加一棵树 $(+1)$，减少一个标记$(-2)$，这是和一步的代价抵消的，因此是均摊 $O(1)$（我们最后加了一个标记）。
2. 对于 Remove Min 操作，我们首先增加了 degree 的势能，然后将 $t$ 棵树合并到 $O(degree)$ 棵树，复杂度 $O(t)$，势能减少 $t-O(degree)$。那这是均摊 $O(degree)$ 的。

因此只剩下最后一个问题：degree 是多少。



##### Degree Analysis

如果一个点现在不是根了，那它之后最多再删一个儿子。

考虑合并的过程。我们给一个 degree 的点加一个儿子时，那个儿子的度数也是 degree。因此一个点的儿子度数（如果不删）还是 $0,1,2,3,\cdots$。那如果删呢？有两种情况：

1. 如果删根的儿子，那度数排序后只会不小于这个序列（归纳即可）
2. 如果删儿子里面的东西，那度数最多减一。

因此可以得到如下结论：

> 对于任意点，其度数第 $i$ 小的儿子度数不小于 $i-2$。其中只考虑儿子数量。

设 $f_d$ 表示度数为 $d$ 的点的最小点数，则 $f_d=1+f_0+\cdots+f_{d-2}$。记 $s_d$ 为序列前缀和，不妨设 $f_{-1}=1$，则 $s_d-s_{d-1}=s_{d-2}$，即刻得到 $s$ 是斐波那契数（$s_{-1}=1,s_0=2$），从而 $f$ 也是。那么度数是 $O(\log n)$ 的。

#### Applications

拿来做 Dij：$O(m+n\log n)$。



Prim：维护一个连通块 $S$，每个点维护 $S$ 到它的最小边权。每次找这个值最小的加进来。

容易发现还是 $n$ 次 Delete Min 和 $m$ 次 Decrease Key。



一个特化的做法是针对 $m$ 很小的情况。此时 $n\log n$ 还是很慢。注意到这个 $\log$ 里面的东西实际上是堆的大小，考虑设一个 $k$，堆里面有 $k$ 个元素就中断。这样的话我们找到了生成树的一部分，里面到外面至少有 $k$ 条边。

考虑从别的点出发继续做，直到每个点出发做一次。如果碰到了之前的点就提前合并。然后我们可以类似 Boruvka 的方式，先把这些合并起来，然后继续做。每个连通块有 $k$ 条向外的边，那么新的点数不超过 $2m/k$。

为了平衡 $m+n\log k$，令 $k=2^{O(m/n)}$，则 $n\leftarrow 2m/2^{O(m/n)}$。记 $s=m/n$，则 $s\leftarrow 2^{O(s)}$。由此可知在 $m=O(n)$ 时，该做法需要 $\log^* n$ 步，复杂度即为 $O(m\log^* n)$。

### III DSU

#### Definition

相信大家都会

##### Path Compression

相信大家都会

##### Merge by Rank

这里的定义是，rank 表示不路径压缩的最大深度，每次合并小的往大的，然后 $rk_a\leftarrow \max(rk_a,rk_b+1)$。

#### Complexity Analysis

首先，rank=d 的节点最多有 $n/2^d$ 个：每次只能两个 $d$ 合成一个更大的。

##### log

注意到每次操作复杂度不超过 $rank$，那么复杂度 $O(n\dot rank)=O(n\log n)$

##### loglog

进行一些势能操作。记一个点的权值是它和父亲的 rank 之差，那么可以证明：

1. 每步操作后，至多只有 $O(\log\log n)$ 个点的权值没有翻倍。

Proof. 如果没翻倍，那么这个点到根的 rank 差就是原来父亲到根的 rank 差的两倍往上，这是叠加的。

那么定义势能为 $n\log\log n-\sum \log(rk_{fa}-rk_u)$（注意到 rk 只有 $\log$）。每步操作只增加势能，每次 union 只改一个势能，因此复杂度 $O(n\log\log n)$

##### log*

设 $T(m,n,r)$ 表示 $m$ 次操作，$n$ 个点，最大 rank 不超过 $r$ 的情况的步数。然后我们按照大小分析操作：

1. $rk_{top}\leq s$，这一部分次数 $T(m_{low},n,s)$。其中 $m_{low}$ 表示这种操作的数量。
2. $rk_u\leq s,rk_{top}>s$。此时代价分成三部分：对于 $rk_u<s-1$ 的点，一步之后它的父亲就更高了。对于分界线上的点，每步只有一个，可以当成下面的 $O(1)$。对于更大的点我们放到下面分析。这里 $n$。
3. $rk_u>s$。注意到这样的点只有 $n/2^s$ 个，可以看成这么多个点上的 DSU。所以 $T(m-m_{low},n/2^s,r)$。

因此得到分析：

$$
T(m,n,r)\leq T(m_l,n,s)+n+T(m-m_l,n/2^s,r)$
$$

那两端减去一个均摊的 $m$，我们需要解

$$
T(n,r)\leq n+T(n,s)+T(n/2^s,r)
$$

我们先有一个 $T(n,r)\leq nr$（第一步分析），那么取 $s=\log r$ 即可得到 $T(n,r)\leq 2n+T(n,\log r)$，因此 $T(n,r)\leq 2n\log^* r$

##### alpha

重写一遍分析。现在就可以取 $s=\log^* r$，得到 $T(n,r)\leq 3n+T(n,\log^* r)$，从而 $\leq 3n\log^{**} r$。这个符号的意思是做几次 $\log^*$ 变成 $1$。

那么可以无限套娃，最后得到 $O(n\alpha(n))$，这个符号有两种意思：

1. 这里的推导：需要 $log$ 上面几个 $*$ 才能变成 $1$。
2. $A(k,1)$ 的反函数。阿克曼函数定义为：

1. $A(0,n)=n+1,A_0(n)=n+1$
2. $A(m,0)=A(m-1,1)$
3. $A(m,n)=A(m-1,A(m,n-1))$。换言之，这相当于把 $A_{m-1}$ 作用 $n+1$ 次，即 $A_m(n)=A_{m-1}^{(n+1)}(1)$。

可以看到 $\log^{***\cdots}$ 是嵌套，这个也是嵌套，所以感性理解这很对。

#### Lowerbound

考虑这样分析：有一棵树，每一步相当于选一条自下向上的路径（可以不到根），然后路径上两两连边，这样之后都可以跳上去。之后选的路径必须是最短路。

为什么可以这样？从下往上合并树，如果操作到一半停下来就先把路径压缩做了再往上合并建树。

然后我们证明如下结论：对于一棵树 $T$，我们可以用二项树的方式扩展出一个 $T(i)$，大小为 $|T|2^i$。然后对于 $s$ 个叶子的树，我们可以在 $T(A(4k,4s))$ 上给每个点做 $k$ 步的操作。

证明：归纳，考虑先造 $k-1$ 然后扩展到 $k$。直接的扩展法是，一步相当于每个点加一个叶子，那把操作放下去就 $+1$ 了。但问题在于，不是所有点都是叶子。

从简单情况开始，考虑 $s=1$。把叶子分为三类：

>             o
>        /  / |
>     o    o  C
>  /  |    |
> o   B    A
> |
> A

A 类叶子占了一半，且它们的父亲都是叶子，那我们可以在 $A(4k-4,4)$ 之后完成 $1/2*1/2$ 的操作。

考虑能不能继续搞。对剩下一半再做一次，那 $$A(4k-4,2^{A(4k-4,4)})<A(4k-4,A(4k-4,O(1)))=A(4k-3,O(1))$$ 步之后，我们就完成了 $3/8$ 的东西。（这里的关键是，复制完之前的操作可以保留）但这样无论如何都做不到 $1/2$ 

再考虑所有的 $B$ 类叶子，它们之间也满足 $A$ 类的关系，那可以把这 $1/4$ 拿出来做一次，然后步数变成再套一次的 $A(4k-2,O(1))$。剩下的就是放缩。

然后考虑多个叶子。那么分类，先做第一个叶子变成 $A(4k,O(1))$ 步，然后做第二个叶子相当于 $$A(4k-4,A(4k-4,A(4k-4,2^{A(4k,O(1))})))$，那就是那种 $A(4k,*)$ 反复嵌套，所以应当是 $A(O(k),O(s))$ 的。

#### General Lowerbound

我们可以证明别的算法也不能比 $\alpha$ 快。

回忆一下之前是怎么证明带修前缀和的：区间分两半，后半段的询问需要知道前半段的若干信息，因此需要 probe 若干个前半段访问过的 cell。

这里考虑这样的合并：每一轮随机二合一（合并完 $n/2^k$ 个集合）。

考虑第 $k$ 轮操作之前。我们可以说明，如果这轮之后的 probe 数量不超过 $O(n/(t2^k))$，那么对于一步不超过 $t$ 个 probe 的随机 find，它大概率需要 probe 一下这一轮最后写的东西。

感性理解：这一轮随机合并信息量是 $n/2^k$ 相关的，如果后面 probe 的太少，那很多信息都没被用到，所以你还是需要 probe 这边找到信息。（为什么具体有个 $t$ 啥的就摆了）

那考虑给序列分 $t$ 段，每一段从一个合并开始，且满足如果这是第 $t$ 轮合并，那么之后的段总 probe 数量加起来不超过 $O(n/(t2^k))$。这样如果在之后加一个随机 Find，期望复杂度就是 $O(t)$。

问题是我们不断加询问必然导致某些段又不满足条件了，因此考虑如下调和的过程：

> 不断加操作，每个操作是一轮随机合并或者一个随机查询。
> 如果当前有 $t$ 段，就加一个查询，否则，新开一段往里面放一个询问。
> 如果加完查询之后有一轮不满足 $n/(t2^k)$ 了，把这一轮和之后的东西全部合并起来，然后重新开始加段。

问题是我们只有 $O(\log n)$ 个合并和 $O(n)$ 个查询。显然上述构造满足查询复杂度都是 $O(t)$ 的，我们只需要再说明复杂度也和放合并的次数有关。此时有如下结论：

1. 如果倒数第 $k$ 段被合并了 $i$ 次，则至多有 $A(2k,2i)$ 轮合并出现。（可能有常数问题）

证明：归纳。考虑倒数第 $k$ 段的情况。一开始这一段只有一个合并，因此它的 $n/2^k$ 是下一段的两倍。那么下一段合并两次，这一段必然合并一次。此时这一段最多有 $A(2k-2,O(1))$ 个合并。

然后就需要下一段合并 $2^{A(2k,O(1))}$ 次，这一段再合并一次，但这个值不超过里面第二个参数加一，所以需要 $A(2k-2,A(2k,O(1)))$，那么可以继续归纳下去，最后还是 $A(2k,2i)$ 之类的。

因此取 $t=O(\alpha(n))$，那么 $O(\log n)$ 个合并下去之后，第一段合并了很多次 $A(O(\alpha(n)),2c)\leq \log n$，但前面取小点 $c$ 就非常大，但每次合并都是新的 $n/t$ 复杂度，所以这边容易搞到很大的复杂度。

否则就是询问放满了，那还是 $O(tn)$。因此有一个 $O(n\alpha(n))$ 的下界。

##### Original Analysis（摆了）

考虑单个询问的复杂度。考虑把 cell 按照上次 probe 的时间分类。如果对于某一类满足：我们把它的值换掉之后答案会变（或者直接超出了 $t$ 次询问），就说明如果答案不变，我们需要 probe 一下这里。注意到“答案”是和这一类里面的修改相关的，但“把它的值换掉”这个事和这一类的询问无关，因此我们只要说明对于一个“把第 $i$ 步probe的东西全部换掉”后的结果，它与很多种“答案”不同，就说明在随一个操作序列的情况下，大概率需要 probe 这里。

更进一步，常见的证明方式是考虑也随机询问，然后把 $n$ 个询问的结果看成 $n$ 维向量。我们需要说明所有可能的答案构成的那堆 $n$ 维向量，和某一个串大概率有很多位不同（不同的维数就是需要 probe 的概率）。然后这通常是一些数学分析。

前缀和的例子也可以这样看：如果某两个询问中间有一个修改，那第二个询问就是完全独立的随机，这样就会有很大的距离。

### IV Splay and LCT

#### Splay

学习 Splay 请自行找 OI 博客。

定义每个点的势能是 $\phi_u=\lfloor\log_2 size\rfloor$，整体势能是加起来。

Thm. 每一次 Splay 操作后，均摊代价不超过 $3(\phi_{root}-\phi_u)+1$，其中包含了深度的 Splay 代价。

Prf. 考虑每一步的操作：

```
Case 1:

   y           x
  / \           \
 x   z   =>      y
                  \
                   z
```

此时代价 $1$，势能增加量不超过原先的 $\phi_y-\phi_x$。这种事情只有最后一步。

```
Case 2:

  z                    x
 / \                  / \
A   y                y   D
   / \     =>       / \
  B   x            z   C
     / \          / \
    C   D        A   B
```

$z$ 的变化是减少，$x$ 的变化是 $\phi_z-\phi_x$，$\phi_y$ 的增加量不超过 $\phi_z-\phi_x$（考虑前后）。因此我们得到了 $2(\phi_z-\phi_x)+1$，但这里再 $+1$ 就无法均摊，我们需要把 $+1$ 吸收进去。

如果 $\phi_z>\phi_x$ 那直接 $2$ 变成 $3$ 就行，但可能会相等。

注意到相等的时候，(AzByCxD) 在某个 $[2^k,2^{k+1})$ 里面，而 (CxD) 的大小也在这里面，相减可得 (AzB) 的 $\phi$ 至少减少了 $1$，这就解决了那个 $1$。

```
Case 3:

  z                    x
 / \                  / \
A   y                z   y
   / \     =>       / \ / \
  x   D            A  B C  D
 / \
B   C
```

类似地得到 $2(\phi_z-\phi_x)+1$。如果 $\phi_z=\phi_x$，此时注意到结束时 $z,y$ 两边子树分离，所以总有一个比 $size_x$ 小至少一半。

然后从上往下加起来即可。

注意到可以给一个非负点权。考虑 $i$ 询问了 $m_i$ 次的情况，直接拿过来作为权值即可得到：复杂度为 $O(m+\sum m_i\log\frac m{m_i})$。这是因为 $\phi_u$ 至少是 $\log m_u$。这说明静态（无修改）下它是渐进理论最优的。

但动态情况是知名的 Dynamic Optimality Conjecture。

#### LCT

学习 LCT 请自行找 OI 博客。

考虑一步 access 的复杂度。每个实边上是 $3(\phi_u-\phi_v)+1$。如果我们把子树大小串起来，那前面就完全消掉了，得到均摊 $O(\log n)$。我们只需要再分析切换的次数。

可以证明这是 $O(m\log n)$ 的：考虑轻重链剖分，易知每次 access 只经过 $O(\log n)$ 条轻边，每次 link/cut 只改变 $O(\log n)$ 条轻边。那么总共切换到轻边的次数是 $O(m\log n)$，因此切换次数也是这个级别。

### V Some Dynamic Graph Algorithms

#### ETT

一个 ETT 是说，我们在树上做 dfs，每次访问和回溯都把当前点加入一次。这样得到一个长度为 $2n-1$ 的序列。

然后考虑怎么做一些操作：

换根：考虑本来是

```
a - b - c - d - e
    |   |   |

(a)-(b)-(c)-(d)-(e)-(d2)-(c2)-(b2)-a-...
```

这里 (b) 和 (b2) 分别表示两边的子树

如果我们换根，就变成了 `(e)-(d)-(c)-(b)-(a)-(b2)-(c2)-(d2)-e-...` 不对这做个锤子。考虑也可以换个顺序变成 `(e)-(d2)-(c2)-(b2)-(a)-(b)-(c)-(d)-e-...`，比对一下可以发现如下算法：

> 找到 $e$ 的第一次出现，从这里拆成两段，把前面扔到后面去。此时开头结尾会有两个旧的根，合并一下。然后在结尾补一个 $e$。

但这还是很难维护：需要找第一次出现。可以发现，选别的 $e$ 做也不是问题：相当于额外把某些 $e$ 的子树移动了一下。因此算法可以改成：

> 找到某一个 $e$，从这里拆成两段，把前面扔到后面去。此时开头结尾会有两个旧的根，合并一下。然后在结尾补一个 $e$。

Link：非常简单，在一边找到 $a$，另外一边换根到 $b$，然后扔到某个 $a$ 之后（再补个 $a$）即可。

Cut: 显然是把一段切出来。但我们需要定位这一段。一种做法是考虑对于每条边 $(u,v)$，记录两次 visit 分别在哪。这样每步操作 $O(1)$ 次修改。

因此我们需要维护：

1. 每个点的某一次出现。
2. 每条边 $(u,v)$ 的两次出现（可以记录出现时前面那个点的位置）

(2) 可以直接维护。(1) 可以维护一个 Set，也可以只记录一个。因为删的时候总是相邻两个 $a$ 合并，所以可以找到第二个。

然后全部序列拿 Splay 维护即可。

可以做子树操作（Euler tour的子区间），但不太好做链操作。这一点和 LCT 正好相反，LCT 的虚边只能向上，所以做不了子树操作。

#### Dynamic Connectivity

维护 $l$ 层子图 $E_i$，在每个图里面找生成森林 $T_i$，满足 

1. 树边是一直出现的：$T_i$ **包含** $T_{i+1}$
2. 非树边是只出现一次的。
3. $E_i$ 中每个连通块大小不超过 $n/2^i$。

这是一个比较趣味的限制：所有树边都是 $T_0$ 依次删边，非树边可以一直往上走，但不能走到树边之上。

怎么维护？加边直接加到 $E_0$，我们只用看怎么删边。一个问题是，删边后我们需要找一条替代边过去，这是 TLE 的。需要一些高端分析。具体来说：

1. 除去直接删边外，维护时只向 $E_i$ 加边。做势能分析。
2. 对于一次删边操作，找到它在树边的最高层 $E_k$，根据性质，它就不在下一层。然后删边，得到两个连通块 $A,B$。此时我们需要枚举到一条 $A$ 连向 $B$ 的边。这很难，一种势能的方式是枚举 $A$ 出发的所有边，如果 $A$ 内部就往上扔，否则就找到了。但扔上去可能它就不是非树边了，因此我们需要把整个 $A$ 里面的树边扔上去。但这真的是可以的：不妨设 $|A|\leq |B|$。此时：根据性质 $|A|\leq n/2^{k+1}$，所以可以直接把 $A$ 这部分扔到上一层。 
3. 如果找到了替换边，就在下面同样换掉。
4. 如果没找到替换边，这一层就连不上了，到下一层继续做。

分析很简单：边只会往上走，性质 $3$ 只用来限制层数，就是 $O(n\log n)$ 次操作。

所有操作用 ETT 维护，需要记录子树内有没有非树边 / 有没有下一层不出现的树边（扔上去用）。

复杂度均摊 $O(m\log^2 n)$

#### Decremental MST

Idea 和上面一样：抄上面的东西，但每一步找替代的时候从小到大枚举边权，然后选最小的加进去就停。

我们只需要证明，在我们找替代边的时候，不会有一条更小的替代边在下面。直观的想，我们每次是把边权小的边放上去，所以边权小的边在上面。

但这只是个直观。实际上我们有：

Thm. 对于任意一个环，环里面边权最大的边在最下面。~~大概跟上面也差不多~~

首先这个显然保证上述要求：把两条替代边组一个环。

考虑证明这个。每次修改唯一需要考虑的是把 $A$ 提上去的部分。对于一个环和里面的最大边，首先最大边不在 $T$ 里面（不然不是 MST）。所以情况只能是这条边在 $A$ 里面，同时因为所有边都至少在这一层，所以整个在 $A+B$ 里面。

1. 如果整个环在 $A$ 里面，那因为从小到大，把这个提上去的时候别的都上去了。
2. 如果不是，那环里面有更小的连接边，所以最大的边上不去。

所以它确实是对的。复杂度不变。

#### Dynamic MST

我们先维护一棵 MST $F$。对于若干条非树边的边集 $S$，我们按照如下方式维护它：

1. 对 $S\cup F$ 维护 Decremental MST. 注意如果之后的操作修改了 $F$，这里不再变动。

有如下定义：

1. 子结构的树边表示“已经考虑过它作为全局树边，不再考虑”；只会考虑非树边作为之后的全局树边。因此，每条全局非树边至少需要在一个结构中作为非树边。

删边：在所有结构中找到替代边。如果 $F$ 需要变，注意到全局最优替代边一定是局部替代边：MST 里面的边在删掉外面一些边后还最优。那么我们一定能找到最优边。但此时剩下的边不满足上一性质了，因此我们用它们和当前的 $F$ 构造一个新的结构。

加边：如果替代就换 $F$，然后把放下来的一条边单独开一个结构。

这样正确性是容易证明的，但复杂度是全错的：有 $O(n)$ 个结构。因此我们加入如下操作：

合并一堆结构里面的非树边，用新的 $F$ 作为树构造一个新的结构。

这个结构满足上述定义，所以这个操作不会改变正确性。现在假设结构复杂度只和 $|S|$ 有关，那我们就有一个很好复杂度的做法：

大小从小到大排序，如果 $2a_i\geq a_{i+1}$ 就合并。

限制保证集合就 $\log$ 个，同时每个集合合并大小乘一个常数，所以均摊代价就多个 $\log$。

这里可以这样实现，也可以正经二进制分组：第 $i$ 个位置可以放一个大小不超过 $2^i$ 的东西，然后向前进位。

那为啥复杂度只和 $|S|$ 有关，树边咋办？在 $F$ 上跑一个虚树（所以它得是静态的）。所以我们需要可持久化 LCT，最直接的方式是 top tree。

每步加进来的替代边数量是 $n\log n$，向上 $\log n$ 层，DecMST 复杂度两个 $\log$，所以加起来四个 $\log$。

#### Vertex-dynamic Connectivity

为了体现与边问题的不同，我们希望这里定义出来暴力做需要 $O(n^2)$ 次边修改。那么可以定义：

> 每个点有开关状态，每次单点修改，查询只考虑开状态的点的连通性。

那反复翻就不能直接套边连通性的做法，但我们还是会使用边连通性。

维护两个集合 P,Q，加进来的点都到 Q 里面去，每 $m^{2/3}$ 轮重构。

P 里面维护连通块，用上面删边连通性的方法维护（注意到上面的方法很容易给出删完拆出来的连通块），将连通块分两类：

1. **原图**度数和至少 $m^{1/3}$ 的，这样只有 $m^{2/3}$ 个，可以和 Q 一起维护。
2. 原图度数和小于 $m^{1/3}$ 的，直接放这么多点很难，但每一个可以看成一些边：把原图所有出边两两相连，这样边数总和 $m^{4/3}$。

于是我们维护另一张图，点是 Q 和 P 里面每一个度数和至少 $m^{1/3}$ 的连通块，然后常规连边。对于 P 里面剩下的连通块，找到它的所有出边，两两连边。边数 $m^{4/3}$，点数 $m^{2/3}$。

考虑一次询问，如果是大度数连通块或者 Q 就直接找到连通性了，否则枚举出边找，$m^{1/3}$。

考虑一次修改，有很多种情况：

1. 删 Q，那就直接 $m^{2/3}$ 暴力。
2. 加点。那加到 Q 里面。Q 和其它点的边只需要 $m^{2/3}$。对于 P 里面小度数连通块，暴力枚举出边更新。注意到一轮里面它们总共的贡献是 $m*m^{1/3}$，所以均摊 $m^{2/3}$。
3. 删 P 里面小连通块，那暴力拆然后删边，均摊还是 $m^{2/3}$。
4. 删 P 里面大的连通块。拆出度数小的连通块都可以暴力做，考虑仍然大的连通块怎么办。一种方式是启发式分裂：把度数和最大的放原地不动，剩下的枚举出边更新。这样总共枚举 $m*polylog$ 条边，即使考虑小度数的影响这里也是均摊 $m^{2/3}$。

#### Another idea

回到最开始的做法。我们的问题是，删边后问两个集合之间有没有边。这不好做。

但考虑一个 UP 的版本：如果有一条边则输出，否则随便。这是好做的：每条边一个随机二进制权值，点权是非树边边权异或，然后 A 里面所有点权的异或，如果只有一条边那异或就是那个值。

我们不知道集合里面有多少边，所以考虑常见规约 UP 做法：随机采样一些边放进去，对于每一个数量都有一个合适的采样数能够对。然后建很多不同采样数的 $E_i$，一起找即可。这样加起来就是最坏期望 polylog。

但有个问题，修改和随机采样不太合适，尤其是我们的修改还是我们随机出来的结果。实际上还是能做，但需要技术。

### VI Flows

定义：众所周知。

#### Augmanting Path Algorithm, Maxflow-Mincut, Ford-Fulkerson

残量网络：大家都会。

Ford-Fulkerson：一直沿着残量网络流。

显然的证明：如果最大流一定不存在残量网络继续流的方式，不存在就是残量网络分开了，就是有一个这样的割。然后有一个割显然流不可能比这个大。

复杂度证明：如果整数边权，则复杂度 $O(nmV)$：跑 $nV$ 轮，每次 $O(m)$。

如果实数边权，那随便找增广路这个事情变得更加极端。实际上我们有：

```
--> a--|
|   v  v
|   b->t
|   ^  ^
s-> c  |
|   v  |
--> d--|

```

我们用 $(a,b,c)$ 表示上中下三条边的正向流量。一开始，我们从 $c\to b$ 流一次把中间的变成 $0$。然后现在是 $(x,0,y)$，假设 $x<y$，那么可以有：

1. 从上往下 $a,b,c,d$ 流，则变成 $0,x,y-x$（注意中间的边反着来的）。
2. 从下往上 $c,b,a$，然后变成 $x,0,y-x$。

这里已经可以看出来一点辗转相减的感觉了，因此类似的，如果 $x>y$，则可以有：

3. 从上往下 $a,b,c,d$ 流，则变成 $x-y,y,0$。
4. 从下往上 $d,c,b$，则变成 $x-y,0,y$。

这构造了一个辗转相减。但随便两个无理数辗转相减就几乎永远不会停，这样这个算法也是不停机。

#### Capacity Scaling

从大到小枚举 $k$。然后每一步有两种做法：

1. 只流 $2^k$ 的整数倍。
2. 只流边权大于等于 $2^k$ 的边，但流的还是实际经过边容量的 $\min$。

但分析都是一样的：一轮之后，存在一个割使得割过去的边权都小于 $2^k$。那么这一次之后剩下的流量不超过 $m2^k$。下一步最多流 $2m$ 次。因此总次数 $O(m\log V)$，复杂度 $O(m^2\log V)$。

#### Edmond-Karp

强多项式做法：用到的数大小是原来大小的 poly 倍，然后假设实数运算 $O(1)$ 的情况下复杂度和 $V$ 无关。

EK: 选一条最短的最短路。

复杂度分析：

记 $s$ 到 $u$ 当前在残量网络上的最短路是 $d_u$。

1. 每步操作所有 $d$ 不降。

证明：每步操作是把一些当前在最短路上的边 $(d\to d+1)$ 翻转，这显然不可能更新 $d$。

2. $m$ 步操作后 $d_t$ 至少增加 $1$。

证明：我们记 admissible graph 为当前的最短路图。然后我们固定当前的 $d$，只考虑能达到 $d$ 的这些边。在 $d_t$ 不变的情况下，我们显然只会用这些边。且如果有一个点 $d$ 变大了，它就不可能再跑到当前的 $d_t$。

那么我们每次跑一条增广路，至少用掉当前图上的一条边（翻转后就不在上面了），这显然只会有 $m$ 次。

这说明总共增广 $O(nm)$ 次。复杂度 $O(nm^2)$。

#### Dinic

考虑一次搞定一个 $d_t$。那么相当于在这个图上增广，但不会加入反向边（距离都更大），然后需要随便一种方式给它流到极大就行。

然后就是经典 Dinic，加个当前弧容易 $O(nm)$。

复杂度 $O(n^2m)$。

#### LCT Flow

我们再考虑上面那个问题。现在考虑每个点选一条入边，用完了再换。这样得到一棵外向树。

那每次流是一个链上最小值，然后区间减。这随便 LCT。

然后我们来考虑怎么换边。假设删掉了 $u\to v$。那么：

1. 如果 $v$ 还有入边，那换进来就行。
2. 否则，$v$ 就没用了，那么直接删点，考虑 $v$ 连向的下一个点，重复这一过程。

每步变成 $O(\log n)$，复杂度 $O(nm\log n)$。

#### Unit Graph: How many properties?

Unit Graph：每条边流量是 $1$，**不准重边**

如果我们用 Dinic 直接跑单位流量的图，那首先一轮变成了 $O(m)$：跑过一条边就 $-1$。

分析1：如果跑了 $n^{2/3}$ 轮，那么此时 $n^{2/3}$ 层里面，一定存在相邻两次点数加起来不超过 $O(n^{1/3})$，此时这两层之间边数不超过 $n^{2/3}$。所以总共就 $O(n^{2/3})$ 轮。

分析2: 换成边数。如果超过 $m^{1/2}$ 轮，那此时至少有一层边数不超过 $m^{1/2}$。

因此有一个 $O(m\min(n^{2/3},m^{1/2}))$ 的复杂度。

#### Binary Blocking Flow

很好的想法: Capacity Scaling 之后差不多是个单位流量。

但实际上还不是：一条边很小那反向边就很大，我们显然不能把大的边拿进来算分层。

那有一个想法：流量大于 $B$ 就看作边权 $0$，否则边权 $1$，这样上面的分析只考虑分层边，所以是对的……但怎么跑 $0/1$-Flow? 直接 dfs 会转圈，因此考虑把 $0$ 边缩 SCC 然后跑 DFS 类算法。但我们还需要考虑还原的问题：一个 SCC 上每个点有一些入度/出度流量，需要还原到 SCC 内部流量。那显然总流量不能太大，否则不可能。可以发现总流量是 $O(B)$（具体来说，$B/2$）还是可以的：先全部流到某个点，然后分别分出去。这样的话，我们每次可以：

1. 推 $B/2$ 的流，或者
2. 推到当前不连通，然后让 $d+1$。

考虑这个怎么分析。先考虑正确性问题。首先，这样边权是动态的，但还是流了一些之后正向距离增大（删边等价于变大到 inf），然后分层一下可以发现 $d(s,u)$ 还是不降的。

然后看怎么 Scaling。我们知道取 $B/2$ 单位的话，边权 $1$ 的边流量都是常数，那么轮数是 $O(\min(m^{1/2},n^{2/3}))$。因此我们要求流量至多是 $O(B\min(m^{1/2},n^{2/3}))$……这不可能。

因此，只拿 $mB$ 分析是不够的，我们需要把流量估计和边权上界分开。具体来说，定义当前的流量上界为 $F$，边权上界为 $B$。我们每一轮只需要给 $F$ 减半。

注意到除了 $mB$ 的下界，我们还有更好的界：$mB/d$，这里 $d$ 是边权处理后的长度。因此另一个轮数是 $2mB/F$。

因此两边加起来是 $2mB/F+2F/B$，从而 取 $F/B=\sqrt m$ 即可让轮数是 $\sqrt m$。又因为单位图有一个 $n^{2/3}$ 上界，取 $n^{2/3}$ 可以让轮数是 $n^{2/3}$。即我们每一轮根据新的 $F$ 确定一个 $B$，然后跑常数流量的问题。一个问题是我们的流可能跑很多小边，最简单的方式是每轮一个 LCT Flow。这样看起来是 $O(m\min(m^{1/2},n^{2/3})\log n\log V)$ 的……

但这真的对吗？我们还需要证明：每一次跑完 Blocking Flow 的时候，$d(s,t)$ 真的加了 $1$。让这个事情不那么显然的问题是，当前有同层的边，所以反向边有可能是能流动的。更近一步，有这个情况：

$$
d_v=d_w,e(v,w)\in[B/2,B],e(w,v)\geq B
$$

这样的话，本来是 $w\to v$ 流过去了 $B/2$ 搞完了 Blocking Flow，但下一步还可以 $v\to w$ 回来，因为它的边权正好变成了 $0$。

但我们可以把这种情况给它毙了：每次变成流 $B/3$，然后把 $e(v,w)\in (2B/3,B),e(w,v)\geq B$ 的边边权也变成 $0$。改成 $B/3$ 是因为那个二倍限制。

#### Number of edges

Thm1. 单位图里面，流量至多是 $n$，使用的边数可以不超过 $O(n^{3/2})$（即使这是个完全图）

Proof. 注意到如果当前流的距离为 $d$，则剩余流量不超过 $(n/d)^2$。那么一条一条流，在剩余流量 $k$ 的时候，距离不超过 $n/\sqrt k$。积分可得 $O(n^{3/2})$.

Thm2. **任意**一个无环流边数都不超过 $O(n^{3/2})$。

Proof. 只考虑这些边的图，如果这里还能调整那一定有环。否则这个图上只有这一个最大流。

然后就有一个无向单位图的做法：每次把无向边缩起来，只保留当前流过的边，然后 bfs。每轮跑完进行一个消圈。

合并无向边和合并点上 $O(n)$ 的走可以跑一个动态图连通性。复杂度 $O(n^{5/2})$。

### VII Cuts

为了凸显和上面的不同，我们经常考虑全局最小割。

#### Global Min-cut

暴力：$O(n*maxflow)$。

随机合并算法：每次随一个非自环，合并。剩两个点的时候把割拿出来。

证明：设最小割为 $C$，考虑一个最小割留下来的概率。在还剩 $n$ 个点的时候，当前每个点度数至少是 $C$（不然割了），那么边数 $nC/2$，从而存活概率 $1-2/n$。那么是 $\prod\frac{n-2}n$，熟知这是 $2/(n(n-1))$。

那么可以跑 $n^2$ 轮，复杂度 $O(n^4)$。

小推论：数数可知最小割不超过 $\binom n2$ 个。而这也是紧的：一个环。

##### Karger's Algorithm

经典技巧：如果我们要搜 $n$ 步操作，直接搜完再做是 $O(nk^n)$ 的，但如果搜一步做一步，并且能快速回溯，那就是 $O(k^n)$ 的。

这里也类似：我们可以把 $n^2$ 条链变成一棵 $n^2$ 个叶子的树，每次先缩一些再向下分裂。

但显然有一个问题：为啥概率还对？提前缩一些会影响公共概率。因此我们需要巧妙地构造。

具体来说，假设当前还剩 $n$ 个点，我们缩到还剩 $n/\sqrt 2$ 个点，然后分裂。容易算出每一步成功概率是 $1/2$，这样分裂到结尾正好 $n^2$ 个点，且复杂度也是 $n^2$……不对缩边复杂度是 $n^2-(n/\sqrt 2)^2$ 地，所以复杂度 $O(n^2\log)$。

然后来看正确概率：给一棵 $n^2$ 个点的二叉树，每条边有 $1/2$ 概率留下，问能连到一个叶子的概率。

然后有十万种算法：比如先直接算，记深度为 $d$，那么随手 $p_{d+1}=1-(1-p_d/2)^2=p_d-p_d^2/4$。

先齐次一下，令 $q_d=p_d/4$，那么 $q_{d+1}=q_d-q_d^2$。然后是：

1. 如果当前在 $1/2$，那么一轮才能降到 $1/4$。
2. 如果当前在 $1/4$，那么两轮（$1/8/(1/16)$）才能降到 $1/8$。
3. 如果当前在 $1/8$，那么四轮（$1/16/(1/64)$）才能降到 $1/16$。

那么即刻发现 $q_d=\Omega(1/d)$，从而概率差不多是 $O(1/\log n)$。所以只需要 $O(\log n)$ 轮。

#### Edge/Vertex connectivity

熟知定义。

我们要求 $k$-VC 至少有 $k$ 个点，不然可能很奇怪。

##### 2-VC and 2-EC

众所周知。

边双：取任意生成树后，非树边在上面并查集合并点。

点双：改成合并边。

拿 DFS 树都非常好做。

##### DFS Tree

没有横叉边。

众所周知的人类智慧。

##### Ear Decomposition

从一个 Cycle 开始，每次加一个里面->外面->回来的路径以扩充图。要求：

Normal ver. 起点终点可以相同，但外面不能重复点（其实也可以删掉）。

Open ver. 所有点不能相同，包括起点终点。

前者对应边双，后者对应点双。证明容易（删掉之后还连通，后者删一个点不影响新的路径）

### VIII Primal Dual

回顾一下 LP，为了记号阳间，这里不写矩阵：

$$
\min \sum_i c_ix_i
s.t. \sum_j w_{i,j}x_j\geq v_i
$$

它的对偶是

$$
\max \sum_i v_iy_i
s.t. \sum_j w_{j,i}y_j\leq c_j
$$

变量和方程对换。

那么一类经典的近似法是，把 IP 的整数限制扬了变成 LP，解出来各种 Rounding 回去。Rounding 丢多少就是近似比。例如如果是 Set Cover 且每个元素只被覆盖 $f$ 次，那一定有一个系数至少是 $1/f$，所以 Rounding 得到 $f$ 倍。

但还有另一个做法。注意到对偶前后最优解是相等的，而对偶又有一个直观的解释：把方程按照系数加起来，覆盖原目标系数的话这就可以作为答案的一个界。

那如果我们把最优解的系数带到对偶里面去呢？得到的组合是理论最紧的，从而那边的任何一个最优解必然卡死了这个。那这就说明：

记 $(x,y)$ 为一组最优解，如果 $x_i>0$，则对应限制 $\sum_j w_{j,i}y_j\leq c_j$ 必然取等。另一方向同理。

拿到 ILP 里面来，现在变成，左边 $x$ 限制整数，右边 $y$ 随便。我们放松限制：

1. 如果 $x_i>0$，那对应的 $y$ 仍然需要满足限制。另一个方向随便。

然后咋搞？初始令 $x,y=0$，那右边都满足，左边都不满足。每个时刻：

1. 我们考虑左边的一个限制 $\sum_j w_{i,j}x_j\geq v_i$。为了满足它，我们希望某个 $x_j$ 变为非零。但这就需要考虑上面的限制，因此我们需要去调整 $y$。注意到这有一个自然的调整对象：$y_i$。
2. 因此我们考虑增大 $y_i$（为了简便，考虑 $w\geq 0$），直到某个时刻，它顶到了某个上界 $\sum_j w_{j,i}y_j\leq c_j$。此时这个方程对应某个 $x_i$。这个 $x_i$ 一定在上面出现，此时我们就可以增大 $x_i$ 了。然后需要看看 IP 怎么说。

#### Set Cover

原 ILP：每个点被覆盖的次数至少是 $1$。

Dual LP：每个点有权值，每个给的集合内权值和不大于集合的 Cost。

翻译一遍上面的东西：

1. 每次我们考虑一个不满足的限制，就是一个还没被覆盖的点 $u$。
2. 我们增大 $y_u$，直到它到达了 Dual 的某个方程（集合）的界。此时选中这个集合。

近似比多少？我们选中的集合都满足条件，那么 $\sum C_j$ 等于每个集合内的点权和。每个点覆盖 $f$ 次，因此这是 $f$ 倍总点权和，就是 $f$ 倍近似。

#### Steiner Tree

2x Approx：回忆一个 TSP 做法：TSP 至少是 MST，而 MST 的 dfs 就是 $2$ 倍的遍历。这里一样：考虑 Steiner Tree 上的一个 dfs 遍历，这样拆出一堆端点之间的路径，总和不超过 $2$ 倍。所以可以只考虑端点间最短路跑个 MST，这不大于上面的路径和。

LP 限制：每个割权值至少是 $1$。指数个？上面的算法不关心这个。

Dual：每个割有权值，每条边总和不超过边权。

翻译一遍，我们每次找不合法的割，给割边加权直到它满。

但每次加哪个割？这不好选。一种折中的方案是：我们把割加边权拆成两个连通块，每个连通块给 $\delta$ 加一半。这样有很多个连通块的时候，我们就可以每个连通块给 $\delta$ 加边权，然后找到第一次连通的东西拿进来。这样好处是我们每次看起来都选了尽量小的。

那做法是容易的。然后怎么分析？只考虑最后选到的那些边，每次加权值可以看成往外扩散的染色。每次向外扩展的总量是当前每个集合向外连的边最后被选的数量总和。

然后就可以发现一些问题：如果有 $10^{114}$ 条重边，那度数炸了，同时不处理一下的话上面的做法一起炸了。

因此考虑最后做一轮删边：如果不影响条件就删了。这样的话上面的总和就是连通块数量的 $2$ 倍不到。从而 $2$ 倍近似。

#### Feedback Vertex Set

删点使得不存在环。

LP 定义：每个环上点权和至少是 $1$。

Dual：每个环有个权值，每个点权值和至多是点权。

直接抄 Set Cover，注意这是反向的，实际上是点覆盖环……那一个环被覆盖了多少次？$O(n)$？

实际上我们只用管选过的点。那么有如下结论：

1. 度数 $2$ 的一条链不用怎么管，因为删一次里面的点就没了。
2. 在任何一个图里面，存在一个只经过了 $O(\log n)$ 个度数 $>2$ 个点的环。

证明：直接 BFS，分叉层数超过 $O(\log n)$ 时至少有 $n$ 个叶子，这不可能，从而必定连到了别的地方。

那么近似比就是 $O(\log n)$。

### IX Matchings

LP：边权，满足点度数和 $\leq 1$。

Dual：点权，满足每条边两边度数和加起来至少是边权。

#### Bipartite version

结论：如果是二分图，则它 ILP 的最优解和 LP 的最优解相同，那么可以直接 Dual。（不然就找个环，一边加一边减）

注意到如果我们能给每个点一个点权，使得：

1. 满足 Dual 限制
2. 只考虑等于的那些边，存在一个完美匹配（先假装没有非负边权）

那整个事情就做完了。

考虑当前做了一半，等于的边构成了一部分连通块。那我们可以整体调整这个连通块的点权，使得它和外面能有新的边加进来。然后我们希望能够增大匹配。

初始时，左边点权 inf，右边点权 $0$。从现在开始，我们只减少左边点权，增大右边点权。考虑一个类似增广的过程：

1. 每次我们从一个没有匹配的点开始，只走等于下界的边，然后向右走非匹配边，向左走匹配边。此时：
2. 如果找到一个右侧没有匹配的点，则增广。
3. 如果走完了，那整个连通块左侧 $-a$，右侧 $+a$。然后存在新的一条边后更新。

复杂度可以是 $\tilde O(nm)$，因为加边可能会有一个 $(m+n\log n)$。

##### Gabow-Tarjan

注意到无权我们可以网络流。带权考虑再上一次 Capacity scaling。但除了上次那个巨大复杂的做法，我们有一个更简单的解决方案。

考虑这样一个版本：

1. 匹配边满足 $w_u+w_v=c_e$。
2. 非匹配边满足 $w_u+w_v\geq c_e-1$，且增广的时候只考虑 $=c_e-1$ 的边。

这样做的一个问题是显然我们最后得到了 $[0,+n]$ 的近似，可以给边权乘 $n$。

复刻上面的增广，唯一区别是每次增广之后，右边整体 $+1$，让匹配边变回等于。但这样的好处是，别的边就可以被扬掉了。

这还有另一种理解方式：一个右侧点只会被 $+1$ 一次：这之后它不存在左侧连过来的 $=-1$ 的边，因为之后左侧再减它也会加。

然后这样做：每轮，所有左侧没匹配的东西来一遍。因为这样的删边，每轮 $O(m)$。

但这样轮数可能很大，因为带权不能直接抄无权 dinic 的 $O(m\sqrt n)$。

因此考虑 Scale，先做边权除以 $2^w$，然后每次多放进来一位，此时保留之前做过的事情。放进来的时候，每个点权变成原来的 $2x+O(1)$ 以覆盖边权。那么每一步之后只多出来 $O(n)$ 的变化量。

然后我们需要类似证明流 $O(n)$ 的东西是 $O(\sqrt n)$ 轮。注意到左边每次没匹配的点全部点权 $-1$，同时右侧最多 $+n$，那么左侧减的量是 $O(n)$，从而 $\sqrt n$ 轮后只剩下 $O(\sqrt n)$ 个未匹配，那么轮数 $O(\sqrt n)$。

复杂度 $O(m\sqrt n\log(nW))$。注意到加边部分可以桶排序，所以不管那么多。

#### General version, Blossom Tree

一般图匹配的 LP 不再整系数：调整遇到奇环就寄，这样只能得到 $0,1,1/2$。

非常好限制：每个奇数大小集合内最多匹配 $\lfloor\frac k2\rfloor$ 条边。那 Dual 的意思是说，每个集合有一个变量（代价是 $\lfloor\frac k2\rfloor$ 倍），然后限制变为 $w_u+w_v$ 再加上所有覆盖这条边的集合权值要不少于边权。

然后我们回忆一下无权的带花树在干啥：

##### Unweighted Blossom Tree

我们像往常一样跑增广路，定义第一个未匹配的点是左侧，从这里开始，左侧走未匹配到右侧，右侧走匹配回到左侧。

但这样可能马上就出现了一个左侧连到左侧的边。这是不可接受的。

在搜索树上看这个东西，它只有 LCA 一个点匹配在奇环外面（也可能没有匹配），同时如果我们希望从这个点向外增广到环的任意位置，那奇环保证总有一个方向增广过去。

因此考虑把这个环缩起来，用 LCA 一个点代替。最后算完匹配后再还原。暴力实现复杂度 $O(nm)$。

然后我们来看带权情况。拿出之前的做法，如果我们增广的时候遇到一个 Blossom，就直接给它对应的新变量 $+a/-a$。

问题是不能减到负数，因此规定遇到右边 $=0$ 的就把它拆了。注意到合并的都在左边，这样一步不会死循环。

复杂度：反正是 poly。

### X Algorithms using FMM

$\omega$：正常域上能做的矩阵乘法是 $n^\omega$。

$\alpha$：$n\times n^{\alpha}\times n$ 的矩阵乘法能 $n^{2+o(1)}$。

注意 $(\min,+)$ 甚至不是一个 Ring，这东西能不能 $O(n^{3-\epsilon})$ 是知名问题。

所有操作都是 Field 运算为单位，所以直接拿 $a\to x^a$ 做 $(\min,+)$ 是不行的。

#### All-pair Reachability

直接 $(A+I)^n$，注意系数别太大（每次 AND 一下）

#### Undirected Unweighted APSP(Seidel)

记 $G^2$ 表示把所有 $(u,v),(v,w)$ 的图里面也连上 $(u,w)$。这可以 $n^\omega$ 计算。

考虑 $G,G^2,G^4,\cdots,G^{2^i}$。那么有：

1. $d_{G^{2^i}}(u,v)\in [2d_{G^{2^{i+1}}}(u,v)-1,2d_{G^{2^{i+1}}}(u,v)]$

证明：不断跳过去，也可以反过来。

那考虑从 $G^n$ 开始不断往回推，直到 $G$。每一次，我们要判定是 $2d$ 还是 $2d-1$。

Lemma1: 如果 $d_2(u,v)$ 是 $2d$，那么 $v$ 相邻的点都满足 $d_2(u,\dot)\geq d_2(u,v)$。

证明：此时原最短路是 $[2d-1,2d+1]$，除完上取整至少是 $d$。

Lemma2：如果是 $2d-1$，那相邻的点都满足 $d_2(u,\dot)\leq d_2(u,v)$，且存在一个点小于。

证明：此时原路径是 $[2d-2,2d]$，至多是 $d$。且考虑最短路上的上一个点就可以得到更小的。

那么如果我们对于每个 $u,v$，求和 $v$ 相邻点的 $d(u,v)$，就能判定。直接求和还是很慢，但这就相当于 $D$ 乘上一个邻接矩阵，所以是 $n^\omega$。

#### Directed, small weight APSP(Zwick)

记最大边权为 $M$。

考虑选一个点， bfs 它向外/向内的距离，然后 n^2 扫一遍。这样我们可以求出所有经过它的最短路。

考虑随机 $\tilde O(k)$ 个点，然后做 $n\times \tilde O(k)\times n$ 的 $(\min,+)$，这样我们可以求出所有长度 $\geq n/k$ 的最短路。

看看这个乘法需要多少时间。单次运算复杂度看起来是 $nM$，窄矩阵的乘法可以用[Coppersmith97] $n^{1.58}k^{0.54}$。

但 $nM$ 还是太大了。考虑分层：做 $k=1,2,4,\cdots,2^i$，这样每一层我们只需要看长度在 $[n/2^i,n/2^{i-1}]$ 之间的。这样的好处是，每次扩展的时候我们只用往外 bfs $O(n/k)$ 条边，从而 $nM$ 变成了 $nM/k$。

那复杂度是 $\max_k M*n/k*(n^{1.58}k^{0.54}+n^{2+o(1)})=Mn^{2.58}k^{-0.46}+Mn^3/k$，就是只考虑前两项。

但如果 $k$ 太小，那暴力就是 $n^2k$ 的，因此可以再做一个平衡，得到啥 $M^{0.68}n^{2.58}$（没有 $n^3/k$ 就是 $2.39$，非常可惜）

#### Approx APSP

直接对着 $(1+\epsilon)^k$ 放缩。

另一种算出来就是 $sth\cdot n^{2.39}$。

#### Witness Product

对于每个 $i,j$，找到一个 $k$ 使得 $A_{i,k}=A_{k,j}=1$。

如果只有一个那大家都会，有多个就上 Sampling，每个 $1/2^i$ 概率来一组。

#### Dominance Product

对于每个 $i,j$，计算 $\{k|A_{i,j}<B_{j,k}\}$。

按照值域分 $k$ 段，不同段之间是矩阵乘法，一段内部暴力。

但这个“一段内部暴力”如果直接枚举是 $O(n^4/k)$，这显然不对：实际上我们固定了一个 $j$，但如果分的太随意就没用。

一种解决方式是，注意到对于不同的 $j$ 问题是分开的：$A$ 的第 $j$ 列和 $B$ 的第 $j$ 行。那我们每一组内部分 $k$ 段，然后每一个第 $i$ 段合并。

复杂度 $O(n^\omega k+n^3/k)=O(n^{(3+\omega)/2})$。

实际上每一个矩阵乘法都可以看成 $n$ 个 $n\times 1\times n$ 叠起来。

#### Sparse Dominance Product

记两边分别有 $n_0,n_1$ 个非零元，复杂度 $O((n_0n_1)^{1/2}n^{(\omega-1)/2})$。

当然是 Fine-Grained 大师做的。

#### Max-Min Product：

$$\max_k\min(A_{i,k},B_{k,j})$$

这就相当于算 $\max_{A_{i,k}|A_{i,k}\leq B_{k,j}}$。

套之前做法的问题是，之前是按照 $A$ 的列分大小，但这里是行内比较大小。

做法1：先按照行分 $k$ 段，然后跑 $k$ 次 $(n^2/k,n^2)$ 的 Sparse Dominance Product，再在每一段里面暴力，复杂度 $O(n^3/k+n^{(3+\omega)/2}\sqrt k)=n^{2+\omega/3}$。

我们重新考虑整体权值划分。这样的好处是跑 Sparse Dominance Product 的时候只用对同一段内部，就是 $(n^2/k,n^2/k)$，然后这边就是 $n^{(3+\omega)/2}$。然后再大的段直接 Boolean Mul，是 $kn^{\omega}$。

问题是一行可能有太多元素（而不是一列！），但 $A$ 的行没那么重要，因此考虑拆行：每 $n/k$ 个元素拆一行，那多 $n$ 行变成 $n^3/k$。总复杂度 $n^{(3+\omega)/2}$。



### XI Parity Game

#### Definitions

有个图，点有整数点权，还有 $A/B$ 的标号。

两个人进行博弈，每次点上标号的人操作，可以走一条出边。

在游戏无限进行的情况下，Infinitely Often 出现的最大权值的奇偶性决定获胜者。

更直观的描述：边权是 $(-1)^kn^k$，然后希望无限走下去之后平均权值是正数/负数。

从直观描述容易发现策略只和当前点有关。然后每个人的策略就是给自己的点找一个出边。

可以发现给一个对手策略，判定能不能赢是一个找能到达的负环问题，所以随便 SSSP。因此这个问题的胜负是属于 $NP\cap coNP$ 的。

但是和质因数分解一样，……

#### Exponential Algorithm

不妨设最大点权对应 $A$ 赢。

找到所有 $A$ 能到达某个最大点权的点，这是一个 oi 常见的变种拓扑排序，或者叫简单博弈求解。

如果这个是全集，那 $A$ 显然必胜：无论如何往最大的点走。

否则我们看剩下这个集合 $S$。注意到走出去对 $B$ 没用甚至更差，所以 $B$ 一定尝试在 $S$ 里面获得胜利。

Lemma. 如果只考虑 $S$，$B$ 能在某个点出发时获得胜利，则原来也能获得胜利。 

Proof. 注意到 $A$ 无论如何走不出去。$B$ 也没必要出去。

如果 $S$ 是空集，那么再次 $A$ 必胜：如果走到 $S$ 就用必胜策略，否则往最大点权走。

否则，记 $B$ 获胜的集合为 $T$，那先找到 $B$ 能走到 $T$ 的部分，这部分显然 $B$ 获胜。对于剩余部分，我们可以说明它们的胜负和外面无关：$B$ 走不到那边，$A$ 走出去必败。

那么每轮我们递归两次，最坏两个都是 $n-1$。复杂度 $O(2^n)$。

#### Subexponential Algorithm

最坏情况是第一步只有一个点，同时第二步能到 $T$ 的只有一个点。

我们解决第二步的情况。枚举所有大小不超过 $c$ 的集合，暴力 check 它们是否满足某个人在里面必胜。全部解决掉之后，第二步就会删掉 $\geq c+1$ 个点。

复杂度 $T(n)=T(n-1)+T(n-c)+O((2n)^c)$。

算一手 lagged fibonacci，得到
$$
\sum_i (2(n-i))^c\sum_j\binom{i-(c-1)j}{j}
$$
最后一项差不多是 $n^{i/c}$，因此平衡得到 $\exp(\sqrt n\log n)$ 级别。

#### Quasipoly Algorithm

Arora-Barak 里面写过的东西：APSPACE(f) $\in$ TIME(2^f)（当然原题比这个劲爆多了，这个是直接暴力）

在这里，博弈自动被 ATM 部分解决了，我们只需要考虑判定谁获胜这件事怎么快速做。换言之，给一个不断往前走的序列，我们希望最小的空间判定谁获胜。



可以证明：博弈相当于，出现环时停止，按照环上最大值判定正负。

为什么？我们只需要证明：有必胜策略的人不会让对手赢。注意到必胜策略使得对手找不到任何一个合法环。





定义一个 $k$-sequence 是一条路径和长度为 $k+1$ 的序列，且其中每相邻两个点中间部分的最大值（包含这两个端点）都是偶数。

显然的结论：存在 $n$-sequence => 获胜。两种奇偶性类似。

但我们显然不能存一个 $k$-sequence，因此我们会用尽量少的信息维护它。但需要满足下面两点：



1. 有必胜策略的人不会让对手赢。
2. 有必胜策略的人自己能赢。

第一条是相对容易的：只要你是个构造 $k$-sequence 的做法，那对手必定不可能构造出来。

但第二条很难：我们只能顺序读入，但这个序列可不是能顺序贪心的。

这里的构造如下：

维护一个 $2^d,2^{d-1},\cdots,2^1,2^0$-sequence，对于每个存在的 sequence，记录一个 $b_i$ 表示它和下一个开头之间的最大值。每次插入时：

1. 如果可以找到一个最大的 $d$，使得 $b_d,\cdots,b_0$ 和插入值同奇偶性，则把它们合并成新的 $2^{d+1}$-sequence，然后替代 $b_{d+1}$。
2. 如果还有比它小的数，找到第一个小于它的数，替代这个 $b_i$ 然后清空后面的 $b_i$（扔到中间的部分）。



第二步是非常暴力的，但我们可以证明，这样保证了有限步内操作结束。



Thm 1. 对于每个 $c$，我们记 $v_c$ 表示把 $\geq c$ 且是偶数的东西看成 $1$，别的看成 $0$，得到的二进制表示的值。那么，如果在两次 $\geq c$ 的偶数操作之间没有别的 $\geq c$ 操作时，$v_c$ 一定会 $+1$：上一次操作之后，序列里面没有 $<c$ 元素，之后的操作：

1. 要么进位了，那最后一次操作必定会去替代那个，所以更大。
2. 否则，前面的位没有变化，同时最后替代了新的一位，所以更大。



然后我们可以证明，这样的事情最多发生 $O(v^n)$ 次：每次这样的事情会让某个 $v_c$ 加一，然后可能任意改动下面的权值，那这可以看成一个 $n$ 进制 counter，因为加 $n$ 次直接赢了。



再然后注意到每 $n$ 步至少经过一个自己的奇偶性，所以每 $n^2$ 步至少经过一个上面的事件，所以总的次数是有限的。

空间是 $\log^2$，所以对应时间 $O(\exp(\log^2 n))$。



### XII Distance Oracle with Failure

问题模型：给一个图，每次询问删去若干点/边，然后问两点最短路。

基础的假设：我们认为任意两点间最短路唯一。一种合适的处理方式是，给每条边随机加一个权值。

为什么不需要 $2^n$ 级别的精度？拿一个最短路 DAG 出来看，使得一个点出发的所有最短路唯一只需要顺序判定 $O(n^3)$ 个不相等的事件。那么 $poly(n)$ 级别的精度就够了。



#### 1 vertex failure

如果删掉一个点 $u$，考虑 $(s,t)$ 的最短路：

1. 如果 $u$ 不在最短路上，那么显然最短路不变。
2. 否则最优解一定是在最短路上绕开一段：从某个点离开最短路，之后再回来。

首先需要判定 $u$ 是否在最短路上，那有最短路唯一的假设之后是非常容易的，只需要算一下 $d(s,u)+d(u,t)$。

然后需要处理在最短路上的情况，此时我们可以通过存 $d$ 提前知道 $u$ 在最短路上的位置。但直接分开看每对 $(s,t)$ 的情况那还是需要存 $O(n^3)$ 个东西。

但放在一起看的话，其实不需要：

对每对 $(s,t)$ 记录：

1. $v_{i,j}$ 表示在前 $2^i$ 个点内脱离最短路，然后在最后 $2^j$ 个点中走回来时的最短路径。
2. $l_i$ 表示正好删掉了从 $s$ 数起第 $2^i$ 个点的最短路。
3. $r_i$ 表示正好删掉了从 $t$ 数起第 $2^i$ 个点的最短路。
4. 最短路上第 $2^i$ 个点。

考虑一个 $(s,u,t)$ 的询问，我们先找到两边的长度 $x,y$，然后找到最大的 $2^i,2^j$。如果最优解是在 $v_{i,j}$ 里面的，那我们就做完了。

不然的话，因为我们找了最大的 $i$，所以脱离的点一定是在 $u$ 到它前面的 $2^i$ 个点中间。此时可以从那个点开始往 $t$ 走，再用 $l_i$：

```
		2^i
|-------------------|
o----t---2^i+1-c--------o
     |------------------|
              2^i
```

另外一边类似，如果有一边不满足那那一边就得到了答案，否则用 $v_{i,j}$。

空间 $O(n^2\log^2 n)$，询问 $O(1)$。

总结：本来很难做的跳出去离 $u$ 很近的情况被用其它点的 $l/r$ 解决了，然后只需要处理很远的情况，那随便放一下。

##### optimization 1

再分析一下上面的过程。用 $l,r$ 解决掉之后，只剩如下情况：跳出去的点在左边离 $u$ 有至少 $2^i$，右边离 $u$ 有至少 $2^j$。之前，我们拿 $v_{i,j}$ 直接覆盖了这种情况。

但还可以更优：记 $L_i$ 表示从前 $2^i$ 个点跳出，然后在 $2^{i+1}$ 个点之后跳回来的最短路。注意到如果选了 $i,j$ 里面较小的一个，那从那边看一定满足这个条件。类似的定义 $R$ 就可以了。

空间 $O(n^2\log n)$，询问 $O(1)$。



##### optimization 2

在最短路树上撒点，给树分块。这样（每个源点分别）剩下 $n/l$ 个点，可以保证每条最短路上这些点把路径切成了一堆长度 $\leq l$ 的段。

现在给一条路径，有两种情况：操作涉及到散块（第一个选中点之前或者最后一个点之后），或者只包含中间部分。

如果删的点在中间，同时我们的路径也是过去再跳，那只需要看关键点到关键点的路径，从而：

1. 只存关键点之间的上述结构，$O(n^2\log n/l)$.
2. 现在的问题是可能在一个小段内，删的点和路径都存在。那每个点额外记录如果删了它，这一段内跳过去要多少距离，从起点跳过它到这一段中间需要多少距离。$O(n^2)$。

那只需要看散块。

1. 如果删的点离一侧不超过 $O(l)$，那可以维护上面的结构，但是到 $i=\log l$ 停止。

还有一种情况：删的点在中间，但我们提前跳了。此时把上面关键点的结构定义换成，从 $s$ 出发，不经过中间的那一段的最短路。这样可以处理 $s$ 出发提前跳，但 $t$ 那边走了关键点的情况。另外一侧类似。如果两边都很小，那删点没有意义，可以预处理。

空间 $O(n^2\log n/l+n^2\log l)$，得到 $O(n^2\log\log n)$。



#### 2 vertices failure

超级大分类讨论。

没感觉课件上写的有任何道理



#### Approx distance with edge failure

**无向图**

取最短路，考虑两边按照 $1+\epsilon,(1+\epsilon)^2,\cdots$ 的距离向外撒点。现在删边完落在一个区间里面，有两种情况：

1. 实际最优解不经过这个区间，那直接记录这个答案即可。
2. 如果经过了这个区间，记删边端点为 $e_1,e_2$，我们考虑求 $u-e_1-v,u-e_2-v$（删一条边情况下）的最短路，这样的话误差最多是这一段的长度级别（无向图可以绕回来），那就是 $\epsilon$ 倍。

对于第二种情况，继续递归做。但幸运的是，有一个终止条件：

3. 如果搞到一半发现最短路分段完出现了重复端点，那就可以直接剪枝。

第一种情况在删多条边的时候也需要递归，一个一个删区间。但这显然也只递归边数次。

但这样递归图就乱了。因此我们考虑：先确定好怎么用这些 $e_i$ 拼路径，然后只递归 Case 1。实现的时候先搞 Case 1 求出所有关键点间两两不经过删边的最短路，然后跑一个关键点上的最短路。复杂度 $O(d^3)$。这样的好处是所有的递归可能只有 $O(n^2\log^d)$ 种。然后把所有可能的递归情况存下来即可。



#### Connectivity with edge failure

判定两个点是否连通等价于判定如下二分图是否存在完美匹配：

1. 每个点拆成 $u_{in},u_{out}$，连一条边。
2. 对于每条边，从 $in$ 连到 $out$。
3. 删掉 $s_{in},t_{out}$。

此时一条路径对应选择这些边的匹配，然后没用到的点用第一部分的边解决。显然相互对应。

因此问题相当于，给一个二分图，每次两边分别删一个点，问是否存在完美匹配。

根据行列式做法加上 Schwatz-Zippel，我们只需要看成一个矩阵，每条边随机权值，然后问行列式是不是 $0$。根据结论如果有完美匹配那满秩的概率不小。

那问题相当于，给一个矩阵，问删掉一行一列后的行列式。相当于求伴随矩阵。

但我们还有删边，这相当于修改矩阵元素。

这可以写成加一个 $UV^T$ 的形式，其中 $U,V$ 都是 $n\times d$。

然后有如下代数结论：

记 $M=\det(A)I+V^Tadj(A)U$，则 $adj(A+UV^T)=\det(A)^{-d}(\det(M)adj(A)-adj(A)Uadj(M)V^Tadj(A))$



证明： Sherman-Morrison-Woodbury formula 和 Sylvester determinant identity。然后因为 $adj=det*inv$。

预处理 $adj$，然后 $M$ 可以 $d^2$ 算，然后右边那堆也非常稀疏。

### XIII Algorithms of FMM

#### Strassen, Tensor Rank

$n^{\log 7}$ 的做法：注意到 $2\times 2$ 矩阵可以线性组合出七个 $(A...)(B...)$，然后加起来得到四个答案。

记这个为 $\langle 2,2,2\rangle=\sum_{i,j,k=1}^2 a_{ik}b_{kj}c_{ji}$（反过来是为了对称性），那上面的算法相当于把它写成了七个 $ABC$ 的和：前面两个代表线性组合的东西，最后一个是贡献系数。

因此定义 Tensor Rank 为，最少能把它写成多少个 $A\times B\times C$ 里面东西的线性组合。

那么 $\log_a R(\langle a,a,a\rangle)$ 就是一个 $\omega$ 的上界。比如如下结论：

$R(\langle 44,44,44\rangle)\leq 36133\rightarrow \omega\leq 2.774$

显然可以 Direct Product，又因为对称性可以轮换，所以给个 $R(\langle a,b,c\rangle)$ 也能做。

#### Border Rank

注意到我们的环不一定是数，也可以是一个多项式。

当然你不能直接操作多项式，所以可以考虑带入一个 $x=\epsilon$，然后假装自己在操作多项式。

但这样的话输出就只能取最低阶项，而如果最低阶项是 $x^0$，那你就和 Tensor Rank 毫无区别……

但最低阶项可以不是 $x^0$。

![1](1.jpg)

这样的算法被称为 Border Rank。

#### Independent Matrices

众所周知 $A+B$ 不一定比分开做难，比如

![](2.jpg)

这个记作 $\langle 4,1,4\rangle\oplus \langle 1,9,1\rangle$。（Tensor 上看，每一个维度上拼接起来）

然后怎么用？大概的想法就是多样的递归，但那样太难说话了。

从 Tensor 角度考虑，把这东西 Pow 几次后，我们能得到很多独立的 $A^iB^{n-i}$，那么 $\max_i\binom niR(A^iB^{n-i})\leq R(A\oplus B)^n$

换一种说法：考虑一堆 $(\binom ni)A^iB^{n-i}$ 大小的矩阵一起乘，那么可以用上面的递归搞出一个计算方式。虽然这里不好说两个大小不同矩阵放一起的影响，但大小相同的还是可以的。

那右边换成我们算出来的值，然后 Bound 左边某一项的 $R$，给出下界。选择最大的一项 Bound，那么只差 $n$ 倍，可以忽略。

举个例子，考虑上面的东西，假设 $R(\langle a,b,c\rangle)\leq (abc)^{\tau}$，那么得到 $\max_i \binom ni (16^i9^{n-i})^{\tau}\leq 17^n$

然后就差不多能得到 $16^\tau+9^\tau\approx\leq 17$，从而 $\tau\leq 0.849,\omega\leq 2.548$。

#### Coppersmith-Winograd

![](3.jpg)

TLDR: 差不多是 $X^0Y^1Z^1+X^1Y^0Z^1+X^1Y^1Z^0$，但这可不是独立的。

然后是一车独立小技巧：

先算 $CW^{N}$，然后只保留主项：$X,Y,Z$ 的系数里面都是 $2n$ 个 $1$ 和 $n$ 个 $0$。

The Salem-Spencer Theorem：存在 $n^{1-\epsilon}$ 大小的一个 $[n]$ 的子集，使得子集中不存在长度为 $3$ 的等差数列。

构造一个 Hash，使得满足上面条件后三维 Hash 出来必定是等差数列。

所以最后（指数上）差不多是 $\binom{3n}{n,n,n}$ 个东西。



#### Higher Order CW Tensor

算的越多越能卡掉一点点。

从现在开始，Hash 做不到（指数上）差不多了，有更多的优化方式。





一个都学不动，摆烂了。

如果这里是我真正的 blog 我大概率会在这里发电，但现在这种情况不适合大量倾倒负面情绪。